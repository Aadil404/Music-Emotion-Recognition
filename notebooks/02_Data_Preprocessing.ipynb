{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN+Z0Jy4cZA/f76bRhTN7dB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aadil404/Music-Emotion-Recognition/blob/main/notebooks/02_Data_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwGexU7wwgjH",
        "outputId": "b8568d0f-5093-4244-d97e-9ffc130509c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/MER(final-year-project)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa) (3.1.0)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from librosa) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.5.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.0.0)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.15.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.1.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.18.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa) (4.5.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.11.12)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.12.1->librosa) (2.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.23)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "# 1. Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Navigate to your project directory (adjust the path if needed)\n",
        "%cd /content/drive/MyDrive/MER(final-year-project)/\n",
        "\n",
        "# 3. Install necessary libraries\n",
        "!pip install librosa tensorflow pandas scikit-learn matplotlib seaborn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# --- 1. Load the Cleaned Multi-Label Metadata ---\n",
        "df = pd.read_csv('data/emotify_dataset/cleaned_metadata_weighted_probabilities.csv')\n",
        "emotion_columns = df.columns[1:] # All columns except 'songs_path'\n",
        "\n",
        "# --- 2. Feature Extraction (Audio -> Spectrogram) with Global Normalization ---\n",
        "def create_mel_spectrogram_segments(audio_path, segment_length=5, hop_length=2.5, target_shape=(128, 216)):\n",
        "    \"\"\"\n",
        "    Create multiple mel spectrogram segments from an audio file.\n",
        "    Uses GLOBAL scaling (-80dB to 0dB) to preserve volume dynamics.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load audio (librosa automatically normalizes audio to -1 to 1 float)\n",
        "        y, sr = librosa.load(audio_path, duration=60)\n",
        "\n",
        "        segments = []\n",
        "\n",
        "        # Calculate segment parameters\n",
        "        segment_samples = int(segment_length * sr)\n",
        "        hop_samples = int(hop_length * sr)\n",
        "        total_samples = len(y)\n",
        "\n",
        "        # Create overlapping segments\n",
        "        for start in range(0, total_samples - segment_samples + 1, hop_samples):\n",
        "            end = start + segment_samples\n",
        "            segment = y[start:end]\n",
        "\n",
        "            # 1. Create mel spectrogram\n",
        "            mel_spec = librosa.feature.melspectrogram(y=segment, sr=sr, n_fft=2048, hop_length=512, n_mels=128)\n",
        "\n",
        "            # 2. Convert to Log-Mel (dB)\n",
        "            # ref=1.0 ensures we are measuring absolute 'loudness' relative to digital full scale,\n",
        "            # NOT relative to the peak of this specific segment.\n",
        "            mel_spec_db = librosa.power_to_db(mel_spec, ref=1.0)\n",
        "\n",
        "            # 3. GLOBAL Normalization (The Fix)\n",
        "            # Music typically ranges from -80dB (silence) to 0dB (max volume).\n",
        "            # We clip values to this range and scale to 0-1.\n",
        "            min_db = -80.0\n",
        "            max_db = 0.0\n",
        "\n",
        "            mel_spec_db = np.clip(mel_spec_db, min_db, max_db)\n",
        "            mel_spec_db = (mel_spec_db - min_db) / (max_db - min_db)  # Scale to 0-1 range\n",
        "\n",
        "            # 4. Resize to target shape (padding if needed)\n",
        "            if mel_spec_db.shape[1] < target_shape[1]:\n",
        "                pad_width = target_shape[1] - mel_spec_db.shape[1]\n",
        "                mel_spec_db = np.pad(mel_spec_db, ((0, 0), (0, pad_width)), mode='constant')\n",
        "            else:\n",
        "                mel_spec_db = mel_spec_db[:, :target_shape[1]]\n",
        "\n",
        "            segments.append(mel_spec_db)\n",
        "\n",
        "        return segments\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {audio_path}: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "0cHZH9G8Qx0C"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = [] # To store spectrogram segments\n",
        "y = [] # To store multi-label vectors\n",
        "segment_song_indices = [] # ðŸ†• NEW: Track which song each segment comes from\n",
        "\n",
        "EXPECTED_SHAPE = (128, 216)\n",
        "\n",
        "print(\"Creating 5-second segments with 50% overlap...\")\n",
        "\n",
        "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
        "    audio_path = 'data/emotify_dataset/' + row['songs_path']\n",
        "    spectrogram_segments = create_mel_spectrogram_segments(audio_path, target_shape=EXPECTED_SHAPE)\n",
        "\n",
        "    if spectrogram_segments is not None:\n",
        "        # For each segment, add to X and repeat the same label\n",
        "        for segment in spectrogram_segments:\n",
        "            X.append(segment)\n",
        "            y.append(row[emotion_columns].values)\n",
        "            segment_song_indices.append(index)  # ðŸ†• Store which song this segment belongs to\n",
        "\n",
        "print(f\"Total segments created: {len(X)}\")\n",
        "print(f\"Original songs: {len(df)}\")\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X = np.array(X)\n",
        "y = np.array(y, dtype='float32')\n",
        "segment_song_indices = np.array(segment_song_indices)  # ðŸ†• Convert to array\n",
        "\n",
        "# Add channel dimension for the CNN\n",
        "X = X[..., np.newaxis]\n",
        "\n",
        "print(f\"Final X shape: {X.shape}\")\n",
        "print(f\"Final y shape: {y.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQM6qUoXX9dP",
        "outputId": "fa2f0ab7-742e-41f9-b955-85aa10f712ef"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating 5-second segments with 50% overlap...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [11:02<00:00,  1.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total segments created: 9152\n",
            "Original songs: 400\n",
            "Final X shape: (9152, 128, 216, 1)\n",
            "Final y shape: (9152, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data while preventing song leakage\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# ðŸ†• Use the precomputed segment_song_indices from Cell 1\n",
        "unique_songs = np.unique(segment_song_indices)\n",
        "train_songs, test_songs = train_test_split(unique_songs, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create masks for segments\n",
        "train_mask = np.isin(segment_song_indices, train_songs)\n",
        "test_mask = np.isin(segment_song_indices, test_songs)\n",
        "\n",
        "X_train, X_test = X[train_mask], X[test_mask]\n",
        "y_train, y_test = y[train_mask], y[test_mask]\n",
        "\n",
        "print(f\"Training segments: {len(X_train)}\")\n",
        "print(f\"Testing segments: {len(X_test)}\")\n",
        "print(f\"Training songs: {len(train_songs)}\")\n",
        "print(f\"Testing songs: {len(test_songs)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JI54pKeXQ0f5",
        "outputId": "4592fad0-0eec-45c9-9d83-fe993e5b4ab5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training segments: 7346\n",
            "Testing segments: 1806\n",
            "Training songs: 320\n",
            "Testing songs: 80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Save for Training ---\n",
        "output_dir = 'processed_data'\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "np.savez_compressed(\n",
        "    os.path.join(output_dir, 'emotify_spectrograms_5s_segments.npz'),\n",
        "    X_train=X_train, y_train=y_train,\n",
        "    X_test=X_test, y_test=y_test,\n",
        "    train_songs=train_songs, test_songs=test_songs,\n",
        "    segment_song_indices=segment_song_indices  # ðŸ†• Save this for reference\n",
        ")\n",
        "\n",
        "print(\"âœ… Preprocessing complete. 5-second segment data saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYU3hdkOYTkk",
        "outputId": "6e07fcc9-4640-4491-9da5-7f110ac56522"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Preprocessing complete. 5-second segment data saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Do1k5WsbAa2",
        "outputId": "a5d46c8e-aa35-4d1d-e98e-27e1f560e7a6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0.21711521],\n",
              "        [0.6955112 ],\n",
              "        [0.86634284],\n",
              "        ...,\n",
              "        [1.        ],\n",
              "        [1.        ],\n",
              "        [1.        ]],\n",
              "\n",
              "       [[0.21711521],\n",
              "        [0.71478605],\n",
              "        [0.891801  ],\n",
              "        ...,\n",
              "        [1.        ],\n",
              "        [1.        ],\n",
              "        [1.        ]],\n",
              "\n",
              "       [[0.21711521],\n",
              "        [0.74457335],\n",
              "        [0.903258  ],\n",
              "        ...,\n",
              "        [1.        ],\n",
              "        [1.        ],\n",
              "        [1.        ]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[0.21711521],\n",
              "        [0.21711521],\n",
              "        [0.32219988],\n",
              "        ...,\n",
              "        [0.21711521],\n",
              "        [0.37285453],\n",
              "        [0.5150104 ]],\n",
              "\n",
              "       [[0.21711521],\n",
              "        [0.21711521],\n",
              "        [0.2948412 ],\n",
              "        ...,\n",
              "        [0.21711521],\n",
              "        [0.37253457],\n",
              "        [0.51486343]],\n",
              "\n",
              "       [[0.21711521],\n",
              "        [0.21711521],\n",
              "        [0.21711521],\n",
              "        ...,\n",
              "        [0.21711521],\n",
              "        [0.37227702],\n",
              "        [0.5146855 ]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpV3gRGLYycE",
        "outputId": "9f4fb215-fef2-4138-8cfa-cc6e2fd0e1be"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.18181819, 0.        , 0.09090909, 0.        , 0.09090909,\n",
              "        0.45454547, 0.09090909, 0.72727275, 0.27272728],\n",
              "       [0.18181819, 0.        , 0.09090909, 0.        , 0.09090909,\n",
              "        0.45454547, 0.09090909, 0.72727275, 0.27272728],\n",
              "       [0.18181819, 0.        , 0.09090909, 0.        , 0.09090909,\n",
              "        0.45454547, 0.09090909, 0.72727275, 0.27272728],\n",
              "       [0.18181819, 0.        , 0.09090909, 0.        , 0.09090909,\n",
              "        0.45454547, 0.09090909, 0.72727275, 0.27272728],\n",
              "       [0.18181819, 0.        , 0.09090909, 0.        , 0.09090909,\n",
              "        0.45454547, 0.09090909, 0.72727275, 0.27272728],\n",
              "       [0.18181819, 0.        , 0.09090909, 0.        , 0.09090909,\n",
              "        0.45454547, 0.09090909, 0.72727275, 0.27272728],\n",
              "       [0.18181819, 0.        , 0.09090909, 0.        , 0.09090909,\n",
              "        0.45454547, 0.09090909, 0.72727275, 0.27272728],\n",
              "       [0.18181819, 0.        , 0.09090909, 0.        , 0.09090909,\n",
              "        0.45454547, 0.09090909, 0.72727275, 0.27272728],\n",
              "       [0.18181819, 0.        , 0.09090909, 0.        , 0.09090909,\n",
              "        0.45454547, 0.09090909, 0.72727275, 0.27272728],\n",
              "       [0.18181819, 0.        , 0.09090909, 0.        , 0.09090909,\n",
              "        0.45454547, 0.09090909, 0.72727275, 0.27272728]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ]
}